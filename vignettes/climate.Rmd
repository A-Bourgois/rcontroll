---
title: "Generate climate data"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Generate climate data}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

TROLL forest simulator relies on climate tables with half-hourly variations of a typical day and monthly variations of a typical year which are recycled through simulation days and years. Initially, TROLL climate tables were computed from the Nouraflux dataset. Variations in quantities of interests (temperatures, ...) were averaged to the target resolution (half-hour for daily variation or month for monthly variation). The purpose of climate generation functions is to compute equivalent climate tables from the ERA5 land reanalysis dataset. With these functions, rcontroll users only need inventories and associated functional traits to run TROLL simulations.

```{r, include = FALSE, eval = FALSE}
knitr::opts_chunk$set(
  eval = FALSE,
  collapse = TRUE,
  comment = "#>"
)
```

This vignette requires numerous packages (12) to work. Some are not integrated to `rcontroll`. Consequently, the chunks are not compiled. But we strongly encourage users that want to generate their climate data for TROLL to install all the packages and run manually this vignette. All the packages role is detailed below. Some may be avoided, only `ecmwfr` is mandatory to download the ERA5 land data from Copernicus and obviously `rcontroll` to convert the data to TROLL inputs.

```{r setup, include = TRUE, eval = FALSE}
library(ecmwfr) # to request data from Copernicus
library(osmdata) # to get bounding box from the study area
library(leaflet) # to make interactive maps
library(terra) # to read the netCDF files
library(lubridate) # to deal with dates and times 
library(dplyr) # to wrangle and tidy the data
library(tidyr) # to wrangle and tidy the data
library(ggplot2) # to make statis maps
library(gganimate) # to make a temporal gif of climate variation
library(nominatimlite) # to get coordinates from the study area
library(sf) # to extract coordinates from spatial objects
library(rcontroll) # to generate TROLL climate inputs
```

# Download ERA5-land data

Before you will be able to download any data you need to get a free personal account and accept the licence to use both ECMWF and Copernicus data.

First ECMWF:

* [Register yourself for ECMWF services](https://apps.ecmwf.int/registration/)

Then Copernicus:

* [Register yourself for CDS services](https://cds.climate.copernicus.eu/user/register)
* [Terms and conditions](https://cds.climate.copernicus.eu/cdsapp/#!/terms/licence-to-use-copernicus-products)

Next you can see your used id (UID) and API key on your account: https://cds.climate.copernicus.eu/user/ .
You need to manually set your account information in R using `wf_set_key` from the `ecmwfr` package with the service `'cds'`.
Beware, you will need the **ECMWF password** to connect to Copernicus services with `wf_set_key`.
Replace the user ID and API key in the following chunk and run it.

```{r,include = TRUE, eval = FALSE}
wf_set_key(user = "******",
           key = "********-****-****-****-************",
           service = "cds")
```

You then define the location to retrieve climate data.
Thanks to the function `getbb` from the package `osmdata` you can directly use the name of the entity, 
for example here the French Guiana for which the default species data have been initialized.
We recommend using a large entity as the request preparation time is often longer than the resulting object to download 
(see estimates below for French Guiana).

```{r,include = TRUE, eval = FALSE}
getbb("French Guiana", format_out = "sf_polygon", limit = 1)$multipolygon %>% 
  leaflet() %>% 
  addTiles() %>% 
  addPolygons()
```

you can then convert the coordinates into the desired request format with `gsub`:

```{r,include = TRUE, eval = FALSE}
(coords <- gsub(",", "/", getbb("French Guiana", format_out = "string", limit = 1)))
```

You need two types of product: (1) monthly averages by hour of day and (2) monthly averages at 00:00.

Now you can use the coordinates to build the request corresponding to the first set of data (monthly averages by hour of day) needed:

```{r, include = TRUE, eval = FALSE}
request <- list(
  "dataset_short_name" = 'reanalysis-era5-land-monthly-means',
  'format'= 'netcdf',
  'product_type' = 'monthly_averaged_reanalysis_by_hour_of_day',
  "variable" = c("10m_u_component_of_wind", "10m_v_component_of_wind", "2m_dewpoint_temperature",
                 "2m_temperature", "surface_pressure", "total_precipitation"),
  "month" = sprintf("%02d", 1:12),
  "time" = sprintf("%02d:00", 0:23),
  "year" = as.character(2012:2022),
  "target" = "ERA5land_hr_FG.nc",
  "area" = coords
)
```

> The names of product and variables can be found directly on the Copernicus website.

Finally you can use `wf_request` to download locally the request with the registered user id (UID):

**Request time:**

**Download size: 199.4 Mb**

```{r, include = TRUE, eval = FALSE}
ncfile <- wf_request(
  user = "152268",
  request = request,   
  transfer = TRUE,  
  path = ".",
  verbose = FALSE
)
```

You can follow your request here : https://cds.climate.copernicus.eu/cdsapp#!/yourrequests .

> As the resquest can be very long you can play with the `time_out` option in `wf_request`. By default it is set to 1 hour, but the request may take longer. We recommand either expanding the time out to expected request time, but you will block you R session (can be useful on cluster if you don't want manual intervention). Or on the oppposite setting a short time out. Then `wf_request` is only used to make the request. And you can download later when ready your request on the Copernicus website (can be useful on local session to avoid blocking the rsession).

You can do the same for the second set of data (monthly averages) needed:

```{r, include = TRUE, eval = FALSE}
request <- list(
  "dataset_short_name" = 'reanalysis-era5-land-monthly-means',
  'format'= 'netcdf',
  'product_type' = 'monthly_averaged_reanalysis', "time" = "00:00",
  "variable" = c("10m_u_component_of_wind", "10m_v_component_of_wind", "2m_dewpoint_temperature",
                 "2m_temperature", "surface_pressure", "total_precipitation"),
  "month" = sprintf("%02d", 1:12),
  "time" = sprintf("%02d:00", 0:23),
  "year" = as.character(2012:2022),
  "target" = "ERA5land_mth_FG.nc",
  "area" = coords
)
```

Finally you can use `wf_request` to download locally the request with the registered user id (UID):

**Request time:**

**Download size: 9.1 Mb**

```{r, include = TRUE, eval = FALSE}
ncfile <- wf_request(
  user = "152268",
  request = request,   
  transfer = TRUE,  
  path = ".",
  verbose = FALSE
)
```

# Have a look to the downloaded data

You can have a look to the resulting ERA5 land data. First extract the chosen value in your file, for instance here total precipitation `tp` and consolidate the data as a table using `dplyr`:

```{r,include = TRUE, eval = FALSE}
test_r <- rast("ERA5land_mth_FG.nc", "tp")
test <- as.data.frame(test_r, xy = T) %>% 
  gather("variable", "value", -x , -y) %>% 
  separate(variable, c("variable", "t")) %>% 
  mutate(date = as_date(time(test_r)[as.numeric(t)])) %>% 
  spread(variable, value)
```

You can then plot the result as a static figure for a chosen year and month using `ggplot`:

```{r,include = TRUE, eval = FALSE}
ggplot(filter(test, t == 1)) +
  geom_raster(aes(x, y, fill = tp)) +
  geom_sf(data = spData::world, fill = NA) +
  scale_fill_viridis_c("Total\nprecipitation", direction = -1) +
  theme_bw() +
  coord_sf(xlim = c(-54.6026945,-51.6346139), ylim = c(2.1109401,5.7769491)) +
  xlab("Longitue") + ylab("Latitude") +
  ggtitle(filter(test, t == 1)$date[1])
```

Or as a dynamic gif using `gganimate`:

```{r, include = TRUE, eval = FALSE }
test %>% 
  group_by(x, y, month = lubridate::floor_date(date, 'month')) %>%
  summarize(tp = sum(tp)) %>% 
  ggplot() +
  geom_raster(aes(x, y, fill = tp)) +
  geom_sf(data = spData::world, fill = NA) +
  scale_fill_viridis_c("Total\nprecipitation\n", direction = -1) +
  theme_bw() +
  coord_sf(xlim = c(-54.6026945,-51.6346139), ylim = c(2.1109401,5.7769491)) +
  xlab("Longitue") + ylab("Latitude") +
  labs(title = 'Month: {frame_time}') +
  transition_time(month)
```

# Prepare TROLL inputs

Next you need coordinates of the location where you want to run your TROLL simulations to extract the data from downloaded ERA5-land data to TROLL climate data.
We will use here the coordinates from the Reserve naturelle des Nouragues where the functional data have been partly collected from.
For that `nominatimlite` provide a useful function called `geo_lite_sf` to easily geocode locations:

```{r, include = TRUE, eval = FALSE}
geo_lite_sf("Réserve naturelle des nouragues, 97301, Régina") %>% 
  leaflet() %>% 
  addTiles() %>% 
  addPolygons(data =  getbb("French Guiana", format_out = "sf_polygon", limit = 1)$multipolygon) %>% 
  addCircleMarkers(col = "red")
```

You can extract corresponding coordinates using `st_coordinates` from `sf`:

```{r, include = TRUE, eval = FALSE}
(coords <- geo_lite_sf("Réserve naturelle des nouragues, 97301, Régina") %>% st_coordinates())
```

You will also need the corresponding time zone for time correction as ERA5-land is giving us time in UTC.
For that you can use the `tz_lookup_coords` function from the package `lutz`:

```{r,include = TRUE, eval = FALSE}
(tz <- lutz::tz_lookup_coords(lon = coords[1], lat = coords[2], method = "accurate"))
```

You can use the `generate_cliamte` function inside TROLL to prepare `TROLL` climatic data:

```{r,include = TRUE, eval = FALSE}
climate <- generate_climate(coords[1], coords[2], tz = tz,
                            era5land_hour = "ERA5land_hr_FG.nc", era5land_month = "ERA5land_mth_FG.nc")
```

And as expected you obtain inputs ready to run models :

```{r, include = TRUE, eval = FALSE}
climate$daytimevar %>% head()
```

```{r, include = TRUE, eval = FALSE}
climate$climatedaytime12 %>% head()
```

```{r, include = TRUE, eval = FALSE}
data("TROLLv3_daytimevar")
list(Nouraflux = TROLLv3_daytimevar,
     ERA5 = climate$daytimevar) %>% 
  bind_rows(.id = "origin") %>% 
  gather(variable, value, -starttime, -endtime, -origin) %>% 
  group_by(origin, variable) %>% 
  ggplot(aes(x = starttime, y = value, col = origin)) + geom_line() + facet_wrap(~variable, scales = "free_y") + theme_bw()
```

```{r, include = TRUE, eval = FALSE}
data("TROLLv3_climatedaytime12")
list(Nouraflux = TROLLv3_climatedaytime12,
     ERA5 = climate$climatedaytime12) %>% 
  bind_rows(.id = "origin") %>% 
  group_by(origin) %>% 
  mutate(month = 1:12) %>% 
  gather(variable, value, -origin, -month) %>% 
  ggplot(aes(x = month, y = value, col = origin)) + geom_line() + facet_wrap(~variable, scales = "free_y") + theme_bw()
```


This is only R objects. `generate_climate` is running fast. But in case you want to run multiple simulations at the same location we recommend saving the corresponding files for later:

```{r, include = TRUE, eval = FALSE}
write_tsv(climate$daytimevar, "ERA5land_daytimevar.txt")
write_tsv(climate$climatedaytime12, "ERA5land_climatedaytime12.txt")
```


